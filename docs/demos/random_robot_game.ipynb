{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Random Policy Robot Game\n",
    "Shows how to use a random policy for an agent and visualize the environment.\n"
   ],
   "id": "7627341fc510471c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set up the environment and prt_rl wrapper\n",
    "Import the JHU robot game and the environment wrapper"
   ],
   "id": "e810743df18dcf7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:52:21.923536Z",
     "start_time": "2025-01-09T19:52:20.033176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from prt_rl.env.wrappers import JhuWrapper\n",
    "from prt_rl.utils.policy import RandomPolicy\n",
    "from prt_sim.jhu.robot_game import RobotGame"
   ],
   "id": "f1126d6bd5b6e9de",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create the environment",
   "id": "da2f8c17bd641c32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:52:21.931405Z",
     "start_time": "2025-01-09T19:52:21.927400Z"
    }
   },
   "cell_type": "code",
   "source": "env = JhuWrapper(environment=RobotGame(), render_mode=\"human\")",
   "id": "67b01c8a03bf751a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialize Random policy\n",
    "Create a random policy that takes in the environment parameters."
   ],
   "id": "620b0a89fa61bd80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:52:21.982909Z",
     "start_time": "2025-01-09T19:52:21.977742Z"
    }
   },
   "cell_type": "code",
   "source": "policy = RandomPolicy(env_params=env.get_parameters())",
   "id": "2ebac474b913915",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run the environment\n",
    "Run the environment for a single episode until it is done."
   ],
   "id": "de0bd098d65ca560"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:52:32.457376Z",
     "start_time": "2025-01-09T19:52:22.034085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reset the environment and set done to False\n",
    "state_td = env.reset()\n",
    "done = False\n",
    "\n",
    "# Loop until environment has reached a done state\n",
    "while not done:\n",
    "    action_td = policy.get_action(state_td)\n",
    "    state_td = env.step(action_td)\n",
    "    done = state_td['next', 'done']\n",
    "    print(f\"State: {state_td['observation']}  Action: {state_td['action']}  Next State: {state_td['next', 'observation']} Reward: {state_td['next', 'reward']}  Done: {state_td['next', 'done']}\")\n",
    "\n",
    "    # Update the MDP\n",
    "    state_td = env.step_mdp(state_td)"
   ],
   "id": "e7d1d86c871daac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([[7]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[7]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[7]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[2]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[7]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[7]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[2]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[5]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[5]], dtype=torch.int32)  Action: tensor([[2]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[7]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[7]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[8]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[8]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[8]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[8]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[5]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[5]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[8]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[8]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[5]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[5]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[1]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[1]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[1]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[1]], dtype=torch.int32)  Action: tensor([[2]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[1]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[1]], dtype=torch.int32)  Action: tensor([[2]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[7]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[7]], dtype=torch.int32)  Action: tensor([[2]])  Next State: tensor([[7]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[7]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[4]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[4]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[0]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[0]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[1]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[1]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[5]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[5]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[5]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[5]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[8]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[8]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[9]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[9]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[9]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[9]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[9]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[9]], dtype=torch.int32)  Action: tensor([[2]])  Next State: tensor([[8]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[8]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[9]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[9]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[10]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[10]], dtype=torch.int32)  Action: tensor([[3]])  Next State: tensor([[10]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[10]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[10]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[10]], dtype=torch.int32)  Action: tensor([[1]])  Next State: tensor([[10]], dtype=torch.int32) Reward: tensor([[-1.]])  Done: tensor([[False]])\n",
      "State: tensor([[10]], dtype=torch.int32)  Action: tensor([[0]])  Next State: tensor([[6]], dtype=torch.int32) Reward: tensor([[-25.]])  Done: tensor([[True]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:52:32.476368Z",
     "start_time": "2025-01-09T19:52:32.474148Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f03104f2be296f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
